# 210811

## dodo tool web

---

倒資料

## 讀書會
---
### Decision and Classification Trees

決策樹與分類樹

### gini impurity

gini 不純度 or 雜質 指標

公式：

$I_G(t) = 1 - \sum^c_{i=1}{p(i|t)^2}$

    1 - 不同結果的預測機率的平方總和

原始的建分類樹：

1. 在所有閥值中選擇 gini impurity 最小的，將閥值加到分類樹中
2. 重新算一次其他的閥值
3. 回到 1.
4. 直到不能再分成更小的群
5. 葉子中占多數結果的為 output

如果葉子中的結果數量很少他的信心度就越少，可以設定每個葉子的最小結果數量。

如果閥值是數值，將所有的數值排序後兩兩取中間值當閥值候選，不過只需要注意候選中最小的 gini impurity 就好。

## Feature Selection and Missing Data

### Feature Selection

為什麼少了胸痛就可以避免 overfit
理由是什麼？多這個特徵 gini impurity 下降得不夠多？

### Missing Data

可以用決策樹也可以用 ML 回歸的方式填充缺失得資料

