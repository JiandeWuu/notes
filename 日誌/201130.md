# 201119

## address model

- test Attention model, self per-training and not per-training 的差異，loss顯示差不多 per-training 的 model 好 0.02 的loss。有可能是因為資料問題，畢竟資料的重複性高。
- per-training bert model，丟下去 training 等待結果。

## tomorrow

- 標注T大的資料
