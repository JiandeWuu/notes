# 22 lee DL 01

$H$: 所有參數的可能性合集

$h^{train}$：由 $D_{train}$ 得到的參數

$$L(h,D)=\frac{1}{N}\sum^{N}_{n=1}l(h,x^{n},\hat{y})$$
___

希望：

$$L(h^{train},D_{all})-L(h^{all},D_{all})\leq \partial$$

What kind of $D_{train}$ fulfil it?

$$\forall h\in H,|L(h,D_{train})-L(h,D_{all})|\leq\partial/2$$

We want sample good train set.
___

計算所有可能 sample 到的 train data 的 loss ，把表現不好的 data set 的機率加總起來

定義 $D_{train}$ is bad:

$$|L(h,D_{train})-L(h,D_{all})| > \varepsilon$$

hoeffding's Inequality:

$$
P(D_{train}\; is \; bad) = \bigcup_{h\in H}P(D_{train}\; is \; bad\; due\; to\; h)\\
\leq\sum_{h\in H}P(D_{train}\; is \; bad\; due\; to\; h)\\
\leq\sum_{h\in H}2\exp(-2N\varepsilon^{2})\\
\leq |H|\cdot 2 \exp (-2N\varepsilon^{2})
$$

$$P(D_{train}\; is \; bad\; due\; to\; h)\leq 2\exp(-2N\varepsilon^{2})$$

- the range of loss L is [0,1]
- N is the number of examples in D_train

___

How to make $P(D_{train}\; is \; bad)$ smaller?

- Larger $N$ and smaller $|H|$

___

## Example

$$P(D_{train}\; is \; bad)\leq |H|\cdot 2 \exp (-2N\varepsilon^{2})$$

$|H| = 10000, N = 100, \varepsilon = 0.1$

> $P(D_{train}\; is \; bad) \leq 2707$

$|H| = 10000, N = 500, \varepsilon = 0.1$

> $P(D_{train}\; is \; bad) \leq 0.97$

$|H| = 10000, N = 1000, \varepsilon = 0.1$

> $P(D_{train}\; is \; bad) \leq 0.00004$

___

Example

$$P(D_{train}\; is \; bad)\leq |H|\cdot 2 \exp (-2N\varepsilon^{2})$$

if we want $P(D_{train}\; is \; bad)\leq \partial$

How many training examples do we need?

$$|H|\cdot 2 \exp (-2N\varepsilon^{2})\leq \partial\Rightarrow N\geq\frac{\log(2|H|/\partial)}{2\varepsilon^2}$$

$$|H| = 10000, \partial = 0.1, \varepsilon = 0.1\\N \geq 610$$

$N$: train set size
___

Model Complexity

$|H|$ the number of possible functions you can select

What if the parameters are continuous?

- Answer 1: Everything that happens in a computer is discrete.

    電腦都是離散的
- Answer 2: VC-dimension(not this coures)

H 不能太小，會導致 loss 變大
