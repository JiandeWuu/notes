# 類神經網路的公式

## Sigmoid
活化

$$\sigma (x) = \frac{1}{1+\exp(-x)}$$

## Softmax
$$y_k=\frac{\exp(a_k)}{\sum^n_{i=1}\exp(a_i)}$$

-	$$y_k$$：是$$k$$類別的輸出機率。
- 	$$a_k$$：是$$k$$類別的計算後的結果。
-	可以算出每個輸出類別的比例（機率），正規化。

## 交叉熵誤差
$$E = -\sum_k{t_k\log{y_k}}$$

-	$$t_k$$：是$$k$$類別的正確結果

## 點間互資訊（Pointwise Mutual Information, PMI）
『**the**』與『car』的**關聯性** 遠遠 **小於 <<<** 『**drive**』與『car』的**關聯性**

但是

『**the**』與『car』的**共生次數**卻 **大於 >>>** 『**drive**』與『car』的**共生次數**
 
使用**點間互資訊**的指標可以解決這種問題。

$$PMI(x,y)=\log_2\frac{P(x,y)}{P(x)P(y)}$$
 
- 	$$P(x)$$代表$$x$$發生的機率
- 	$$P(y)$$代表$$y$$發生的機率
-   	$$P(x,y)$$是指$$x$$與$$y$$共生的機率
-    PMI的值越高，代表關聯性越高

$$PMI(x,y)=\log_2\frac{P(x,y)}{P(x)P(y)}=\log_2\frac{\frac{C(x,y)}{N}}{\frac{C(x)}{N}\frac{C(y)}{N}}=\log_2\frac{C(x,y) \cdot N}{C(x)C(y)}$$

-	假設共生矩陣為$$C$$
- 	$$C(x, y)$$代表字詞$$x$$與$$y$$的共生次數
-  	$$C(x)$$、$$C(y)$$表示字詞$$x$$與$$y$$出現的次數
-   	語料庫內含的字詞數量為$$N$$

## 餘弦相似度
表示的是『兩個向量朝同方向的程度』

當有2個向量 $$x$$,$$y$$

$$x = (x1, x2, x3,...,xn)$$

$$y = (y1, y2, y3,...,yn)$$

$$similarity(x,y) = \frac{x\cdot y}{\left\|x\right\|\left\|y\right\|} = \frac{x_1y_1+\cdots+x_ny_n}{\sqrt{x^2_1+\cdots+x^2_n}\sqrt{y^2_1+\cdots+y^2_n}}$$

## Negative Sampling 的取樣機率
$$P'(w_i) = \frac{P(w_i)^{0.75}}{\sum^n_j{P(w_i)^{0.75}}}$$

-	$$P(w_i)$$代表第$$i$$個字詞的機率