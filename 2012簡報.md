# 2012 簡報

## 問題 - 地址解析

- 輸入：一串地址（ex:高雄市前鎮區中安路1號）
- 輸出：依照行政層級（縣市鄉鎮區路段...）分類，{City:高雄市, Dist:前鎮區, IdLocation:中安路, AddressNumber:1號}

為什麼需要這麼做？

- 避免使用者輸入奇怪的東西
- 幫助後續的處理提取有用的資訊
  
## 方法

- 正則表達式
  <!-- - 第一個想到方法，也是普遍的使用方法。 -->
  - 在使用時會幫輸入製作一串規則，`(\S*)(\D{2}[市縣])(\D{1,2}[市區鄉鎮]|\D{3}[鄉鎮])(\D{2,3}[村里])?(\S+[鄰])?([^路]{2,5}[路道街]{1})?(\S{1,2}[段])?(\S+[巷])?(\S+[弄])?([0-9一二三四五六七八九十]+[號]|[0-9一二三四五六七八九十]+[-|之][0-9一二三四五六七八九十]+[號]?)(\S*)`，如果輸入有其他種規則的話就需要另外製作一串的規則。
  - 規則太多的話，難以窮舉出所有的規則與邏輯去製作。
  - 需求有變，不管是輸入還是輸出都需要重新製作規則。
  <!-- - 對於口語化的輸入難以製作規則。（文心路三段寶雅旁邊） -->
- 模糊查詢
  - 在現有資料庫進行特定資料與輸入字段的查詢，`%高雄%`。
  - 每一次的查詢都需要查詢整個資料庫，會依照輸入字串的文字數量導致查詢負擔的上升。
- NLP-NER
  - 如上述的基於規則的方法、基於字典的方法
  - HMM，CRF：傳統的機器學習的模型
  - RNN-CRF，LSTM-CRF：深度學習的模型
  - 注意力模型、遷移學習（pre-train就是其中的一個動作手法）、半監督學習的方法

## 經歷

- 擁有的資料
  - 內政部
    - 正規
    - 順序調換
    - 缺某些 tag
  - 另外（少量)
    - 不是 NER 的目標 例如 多餘字 非地址
    - 真實使用者輸入

- BiLSTM 雙向長短期記憶模型
  - 是什麼
    - 字面解釋，長短期記憶：不但只參考靠近它的字也會考慮離它很遠的字。雙向：會從頭讀到尾也會從後面讀到頭。
  - 預期結果
    - 完整的參考整段文字，理解文意並正確將所需的資訊標記出來
  - 結果
    - 不管使用哪種單一**規則**訓練都有良好的表現。(規則需補充)
    - 能夠將訓練的規則準確的判斷出來。
  - 不能解決什麼
    - 無法對其他的規則進行判斷。
    - 以地址為例？？？？
    - 不過其實這件事是合理的，你不能期待一個只學過微分的人做積分也可以很厲害。
- Bert
  - 是什麼
    - google 提出的一種 Transformers 模型，使用維基百科的資料 pre-training。
    - pre-training: 預先訓練，在某種程度上得到字詞與字詞之間的關係與含意，可以想像一下在某個空間中字詞與字詞的距離就是他們的關係。
  - 預期結果
    - 以地址解析為例，期待可以正確判別出不是地址的部份，將備註或者其他的文字判斷出來。
    - ex: 台中市北區文心路 寶雅(POI) (實況主amber) 前面是地址 後面是非地址的部份 
  - 結果
    - 在正規資料的表現不如 BiLSTM，在特殊資料（有備註的資料）有稍微改善。（解釋正規資料，規則問題要小心）
    - 在規則比較多樣的資料訓練出來的模型，對正規資料自己訓練出來的模型表現還好。
    - 訓練時間太久。
  - 不能解決什麼
    - 與 Bilstm 一樣

- self Pre-training
  - 是什麼
    - 使用訓練資料進行預訓練，先了解資料文字關係。
  - 使用理由
    - 既然是預先訓練出字詞關係、字詞向量，那直接用訓練資料進行預訓練應該要有更好的效果。我只需要知道訓練資料的文字之間關係就夠了，說不定就是因為太多字詞關係導致效果不好。
  - 結果
    - Bert
      - 雖然有稍微的改善但是卻還是比不上 BiLSTM 的表現。
- BiLSTM self Pre-training
  本來效果就很好，所以實驗 `self pre-training` 對文字的理解是否真的有效。
  - 實驗：文字理解能力
    - 是什麼
      - 將本來預期就很好的 `BiLSTM` 加上 `self pre-training` 是不是可以更好更有泛化能力。
      - 如果我們開始使用真實資料訓練了，資料一定不多要怎麼將不多的資料訓練出來的能力擴大到全台。
    - 實驗方式
      - 使用全台訓練資料進行 pre-training。訓練使用單一或是少數縣市資料。評估使用全台驗證資料與真實資料。
    - 預期結果
      - 使用單一或是少數縣市就可以準確預測全台資料。
      - 如果表現的好，也就代表之後收集的真實資料就算都在某個縣市，我們也可以拿到一個對全台真實資料效果不錯的模型。
    - 結果
      - 單一就有很不錯的準確率了，給2個縣市甚至到95%以上。
      - 所以 self pre-training 是一個對文意理解很有效的方法，後續可以繼續使用。

## 準備

- 前處理
  - 全形，半形
  - 中文數字，數字
  - 大小寫
  - 之 -

## 實驗

[] taiwan pre training，transformers train
[] 前處理成同一規範（中文數字取代成數字，全形轉半形，大寫轉小寫）


# 有沒有作到
地址 人看的像地址
順序調換
欄位沒給
多餘字
訓練資料沒有的字詞


pre-train, fine-tuning, valid(yiyu), test(yiyu), valid(全台), test(全台)
bert(全台), bert(全台),
bilstm(全台), bilstm(全台),
bilstm(全台), bilstm(新竹、宜蘭),
no, bert(全台)
no, bilstm(新竹、宜蘭),

Q: 這裡的資料，例如：(全台)，是有順序調換或抽掉欄位的嗎？
(順序調換
欄位沒給
多餘字
訓練資料沒有的字詞) 有沒有這幾種之一的屬性
  