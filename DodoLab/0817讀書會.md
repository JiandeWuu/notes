# 0817 讀書會

### Decision and Classification Trees

決策樹與分類樹

### gini impurity

gini 不純度 or 雜質 指標

公式：

$I_G(t) = 1 - \sum^c_{i=1}{p(i|t)^2}$

    1 - 不同結果的預測機率的平方總和

原始的建分類樹：

1. 在所有閥值中選擇 gini impurity 最小的，將閥值加到分類樹中
2. 重新算一次其他的閥值
3. 回到 1.
4. 直到不能再分成更小的群
5. 葉子中占多數結果的為 output

如果葉子中的結果數量很少他的信心度就越少，可以設定每個葉子的最小結果數量。

如果閥值是數值，將所有的數值排序後兩兩取中間值當閥值候選，不過只需要注意候選中最小的 gini impurity 就好。

## Feature Selection and Missing Data

為什麼少了胸痛就可以避免 overfit
理由是什麼？多這個特徵 gini impurity 下降得不夠多？

### Missing Data

可以用決策樹也可以用 ML 回歸的方式填充缺失得資料

## Random Forests

擁有決策樹的簡潔性與靈活性
因為決策樹的在處理新的資料時表現不夠好。

### Building

1. Create a "bootstrapped" dataset.
    
    用亂數取得新的 dataset 可以重複選擇某一個 sample 
2. Create a decision tree using the bootstrapped dataset, but only use a random subset of variables (or columns) at each step.

    使用剛剛建的 bootstrapped dataset 建一個分類樹，但是每次只隨機看某幾個參數欄位。 
3. 回到第一步重複建立 bootstrapped dataset 與分類樹，依照影片的説法是 100 次

### Using

將要預測的資料放進 Building 出來的全部 100 顆的 Random Forests 記錄所有的輸出結果，以最多的結果為輸出。

### Evaluating

因為 bootstrapped 是可以重複的隨機亂數取得的 dataset 所以會有在 building 沒使用到的 data 就可以用來評估。

可以做評估就可以改參數來改善結果，可以改 building step 2 每次看的參數欄位數量或者是總共要建立的樹的數量。

### Missing data

少了某些參數欄位的資料。

#### 沒有少結果欄位

1. 依照結果欄位分析，如果是布林欄位就用相同結果的最多種類參數為值，如果是數值欄位則是用相同結果的中位數。
2. Building Random Forests
3. Using Random Forests output The proximity matrix
4. 將本來 Missing 的參數用 proximity matrix 的權重加權後填充回去。

#### 也少了結果欄位

將結果的可能性列出來，依照沒有少結果欄位的方法去填充資料，將每種結果的答案都在做一次 Using 正確率高的當作正確資料。
