# 至本週三預計執行

1. 模型
   1. 研究
      1. 中文詞性與分詞
      2. 建宏推薦的書
      3. 其他
   2. 開發
      1. 評估函式
   3. 訓練
      1. BiLSTM + CRF
         1. rand_r1
      2. Bert + CRF
         1. rand_r1
   4. 目前的所有規則作表格對比（Bilstm CRF、bert)
2. 佈署測試（空）

## 執行項目

1. 模型
   1. 研究
      1. 中文詞性與分詞
         1. Adversarial Multi-Criteria Learning for Chinese Word Segmentation(對抗性多標準學習的中文分詞) <https://github.com/jzxc56788/node/blob/master/read_paper/Adversarial%20Multi-Criteria%20Learning%20for%20Chinese%20Word%20Segmentation.md>
   2. 開發
      1. 評估函式（?）
         1. Bert + CRF, 正常
         2. BiLSTM + CRF, Loss低但是Accuracy也低
   3. 訓練（TWCC 開了2個機器）
      1. BiLSTM + CRF
         1. regular_r1_train（預計10/27 23:00 結束）
      2. Bert + CRF
         1. regular_r1_train（完成）
         2. rand_r1_train（完成）
         3. mask_r1_train（完成）
   4. 評估
      1. BiLSTM + CRF
      2. Bert + CRF
         1. regular_r1
            1. regular_r1_train（完成）
            2. regular_r1_valid（進行中）
            3. rand_r1_valid（待）
            4. mask_r1_valid（待）
            5. TD_test_r1（完成）
         2. rand_r1（待）
         3. mask_r1（待）
   5. 評估表格（Bilstm CRF、bert)

      <https://docs.google.com/spreadsheets/d/1gxPXDFw-THxi1yRzktRKvVq4DK_OsDjYnJm6oDYJfRw/edit?usp=sharing>

2. 佈署測試（空）

- regular_r1：阿達提供的正規資料。
- rand_r1：用regular_r1的每筆資料隨機做前後調換順序。前後調換順序：是將路名以後的含路名換到最前面，來模仿google輸出的格式
- mask_r1：用regular_r1的每筆資料隨機刪除一個已有的類別

## 至下週三預計執行

1. 模型
   1. 研究
      1. 中文詞性與分詞
   2. 開發
      1. 評估函式（解決）
   3. 訓練（TWCC 開了2個機器）
      1. BiLSTM + CRF
         1. regular_r1_train（預計10/27 23:00 結束）
      2. Bert + CRF
         1. self-pertraining
            1. regular_r1_train
            2. rand_r1_train
            3. mask_r1_train
   4. 評估
      1. BiLSTM + CRF
      2. Bert + CRF
         1. regular_r1
            3. rand_r1_valid（待）
            4. mask_r1_valid（待）
         2. rand_r1（待）
         3. mask_r1（待）
2. 佈署測試（空）
